{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 3**\n",
    "\n",
    "Due: **October 8th, 5pm** (late submission until October 11th, 5pm -- no submission possible afterwards)\n",
    "\n",
    "Written assignment: 10 points\n",
    "\n",
    "Coding assignment: 25 points\n",
    "\n",
    "Project report: 15 points\n",
    "\n",
    "### Name: [Yawen]\n",
    "\n",
    "### Link to the github repo: [https://github.com/IsabellaTan/Brown-DATA2060-HW3.git]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written assignment\n",
    "\n",
    "### **Gradient Descent (10 points)**\n",
    "\n",
    "Consider using gradient descent to find the minimum of $f$, where,\n",
    "- $f$ is a convex function over the closed interval \\[-*b*,*b*\\], *b* > 0\n",
    "- $f'$ is the derivative of $f$\n",
    "- $\\alpha$ is some positive number which will represent a learning rate parameter\n",
    "\n",
    "The steps of gradient descent are as follows:\n",
    "\n",
    "- Start at $x_{0} = 0$\n",
    "- At each step, set $x_{t+1} = x_{t} - \\alpha f'(x_{t})$\n",
    "- If $x_{t+1}$ falls below  -*b*, set it to -*b*, and if it goes above *b*, set it to *b*.\n",
    "\n",
    "We say that an optimization algorithm (such as gradient descent)\n",
    "*$\\epsilon$-converges* if, at some point, $x_{t}$ stays within  $\\epsilon$ of\n",
    "the true minimum. Formally, we have *$\\epsilon$-convergence* at time $t$ if\n",
    "\n",
    "$\\quad \\quad |x_{t'}-x_{\\min}| \\le \\epsilon, \\quad \\text{where } x_{\\min}=\\underset{x \\in [-b,b]}{argmin} f(x)$ for all $t' \\geq t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 1**\n",
    "For $\\alpha$ = 0.1, *b* = 1, and $\\epsilon$ = 0.001, find a convex function $f$ so that running gradient descent does not  $\\epsilon$-converge.\n",
    "Specifically, make it so that *x*<sub>0</sub> = 0,\n",
    "$x_1$ = *b*,  $x_2$ =  - *b*,  $x_3$ = *b*,  $x_4$ =  - *b*, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "Define function f(x) = 15 $x^2$ - 10x over the close interval [-1, 1] where b = 1\n",
    "\n",
    "Firstly we check if f(x) is convex. \n",
    "\n",
    "Since f(x) = 15 $x^2$ - 10x, then f'(x) = 30x - 10 which monotonically nondecrease, and f''(x) = 30 which is nonnegative, so that f(x) is convex function. \n",
    "\n",
    "Secondly we check if gradient descent doesn't $\\epsilon$-converge.\n",
    "\n",
    "Since f'(x) = 30x - 10 and $x_{t+1} = x_{t} - \\alpha f'(x_{t})$ and $\\alpha$ = 0.1, then $x_{t+1} = x_t - 0.1(30x_t -10)$ = $-2 x_t$ + 1\n",
    "\n",
    "Let $x_0$ = 0\n",
    "\n",
    "$x_1$ = $-2 x_0$ + 1 = 1\n",
    "\n",
    "$x_2$ = $-2 x_1$ + 1 = -1\n",
    "\n",
    "$x_3$ = $-2 x_2$ + 1 = 3 = 1 because when goes above b, set it to b\n",
    "\n",
    "$x_4$ = $-2 x_3$ + 1 = -1\n",
    "\n",
    "$x_5$ = $-2 x_4$ + 1 = 3 = 1\n",
    "\n",
    ".....\n",
    "\n",
    "Since $x_i$ is always equal to 1 or -1, then f gradient descent doesn't $\\epsilon$-converge.\n",
    "\n",
    "Thus, convex function f(x) = 15 $x^2$ - 10x so that running gradient descent does not  $\\epsilon$-converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "For *$\\alpha$* = 0.1, *b* = 1, and  $\\epsilon$ = 0.001, find a convex function $f$\n",
    "so that gradient descent does *$\\epsilon$-converge*, but only after at least\n",
    "10,000 steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** \n",
    "\n",
    "Define function f(x) = $0.001 (x-1)^2$ over the close interval [-1, 1] where b = 1\n",
    "\n",
    "Firstly we check if f(x) is convex. \n",
    "\n",
    "Since f(x) = $0.001 (x-1)^2$, then f'(x) = 0.002(x-1) which monotonically nondecrease, and f''(x) = 0.002 which is nonnegative, so that f(x) is convex function. \n",
    "\n",
    "Secondly we check if gradient descent does $\\epsilon$-converge for $\\epsilon$ = 0.001\n",
    "\n",
    "Since f(x) = $0.001 (x-1)^2$, then $x_{min}$ = 1, so that $\\epsilon$-convergence at time $t$ is $|x_{t'}-1| \\le 0.001$ for all $t' \\geq t$.\n",
    "\n",
    "Let $d_t = |x_{t}-1|$ be the distance between $x_t$ the minimum ($x_{min}$ = 1)\n",
    "\n",
    "Since f'(x) = 0.001x and $x_{t+1} = x_{t} - \\alpha f'(x_{t})$ and $\\alpha$ = 0.1, then $x_{t+1} = x_t - 0.0002(x_t - 1)$, so that we can get:\n",
    "\n",
    "-> $x_{t+1} -  x_t$ = -0.0002($x_t$ - 1)\n",
    "\n",
    "-> $x_{t+1}$ ​− 1 = 0.9998($x_t$ ​−1)\n",
    "\n",
    "Initial point: $x_0​=0, d_0 ​= ∣x_0​−1∣ = ∣0−1∣ = 1$\n",
    "\n",
    "Step 1: $x_1​−1 = 0.9998(x_0​−1) = 0.9998(−1) = −0.9998$, then $d_1 ​= ∣x_1​−1∣ = ∣−0.9998∣ = 0.9998$\n",
    "\n",
    "Step 2: $x_2​−1 = 0.9998(x_1​−1) = 0.9998(−0.9998) ≈ −0.9996$, then $d_2 = 0.9996$\n",
    "\n",
    "Step 3: $x_3​−1 = 0.9998(x_2​−1) = 0.9998(−0.9996) = 0.9998 * 0.9998 * (−0.9998) ≈ −0.9994$, then $d_3 = 0.9994$\n",
    "\n",
    "......\n",
    "\n",
    "We can find $d_t = 0.9998^t$ which is the distance between $x_t$ the minimum.\n",
    "\n",
    "When $d_t \\le 0.001$ , $0.9998^t \\le 0.001$. Then when $0.9998^t = 0.001$, t ≈ 34500 > 10000 steps\n",
    "\n",
    "Thus, convex function f(x) = $0.001 (x-1)^2$ so that gradient descent does $\\epsilon$-converge after 34500 steps (>10000 steps). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Coding Assignment (25 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the evironment test below, make sure you get all green checks. If not, you will lose 2 points for each red or missing sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.12.11\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.10.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 2.3.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.7.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 2.3.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pytest version 8.4.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m torch version 2.7.1 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.11\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.11\"):\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.10.5\", 'numpy': \"2.3.2\",'sklearn': \"1.7.1\", \n",
    "                'pandas': \"2.3.2\", 'pytest': \"8.4.1\", 'torch':\"2.7.1\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this assignment, you will be using a modified version of the UCI\n",
    "Census Income data set to predict the education levels of individuals\n",
    "based on certain attributes collected from the 1994 census database. You\n",
    "can read more about the dataset here:\n",
    "[`https://archive.ics.uci.edu/ml/datasets/Census+Income`](https://archive.ics.uci.edu/ml/datasets/Census+Income).  \n",
    "\n",
    "### Stencil Code\n",
    "\n",
    "We have provided the following stencil code within this file:\n",
    "\n",
    "-   `Model` contains the `LogisticRegression` model you will be\n",
    "    implementing.\n",
    "\n",
    "-   `Check Model` contains a series of tests to ensure you are coding your \n",
    "    model properly.\n",
    "\n",
    "-   `Main` is the entry point of program which will read in the\n",
    "    dataset, run the model, and print the results.\n",
    "\n",
    "You should not modify any code in `Check Model` and `Main`. If you do for debugging\n",
    "or other purposes, please make sure any additions are commented out in\n",
    "the final handin. All the functions you need to fill in reside in this notebook,\n",
    "marked by `TODO`s. You can see a full description of them in the section\n",
    "below.\n",
    "\n",
    "### **The Assignment**\n",
    "\n",
    "In `Model`, there are a few functions you will implement. They are:\n",
    "\n",
    "-   `LogisticRegression`:\n",
    "\n",
    "    -   **train()** uses stochastic gradient descent to train the\n",
    "        weights of the model.\n",
    "\n",
    "    -   **loss()** calculates the log loss of some dataset divided by\n",
    "        the number of examples.\n",
    "\n",
    "    -   **predict()** predicts the labels of data points using the\n",
    "        trained weights. For each data point, you should apply the\n",
    "        softmax function to it and return the label with the highest\n",
    "        assigned probability.\n",
    "\n",
    "    -   **accuracy()** computes the percentage of the correctly\n",
    "        predicted labels over a dataset.\n",
    "\n",
    "*Note*: You are not allowed to use any packages that have already\n",
    "implemented these models (e.g. scikit-learn). We have also included some\n",
    "code in `main` for you to test out the different random seeds and\n",
    "calculate the average accuracy of your model across those random seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logistic Regression**\n",
    "\n",
    "Logistic Regression, despite its name, is used in classification\n",
    "problems. It learns sigmoid functions of the inputs\n",
    "$$h_{\\bf w}(x)_j = \\phi_{sig}(\\langle {\\bf w}_j, {\\bf x} \\rangle)$$\n",
    "where $h_{\\bf w}(x)_j$ is the probability that sample\n",
    "$\\bf x$ is a member of class *j*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multi-class classification, we need to apply the `softmax` function\n",
    "to normalize the probabilities of each class. The loss function of a\n",
    "Logistic Regression classifier over *k* classes on a *single* example\n",
    "(*x*,*y*) is the **log-loss**, sometimes called **cross-entropy loss**:\n",
    "$$\\ell(h_{\\bf w}, ({\\bf x}, y)) = - \\sum_{j = 1}^{k}\n",
    "\\left\\{\\begin{array}{lr}\n",
    "    \\log( h_{\\bf w}({\\bf x})_j ), & y = j\\\\\n",
    "    0, & \\text{otherwise} \\\\\n",
    "\\end{array}\\right\\}$$\n",
    "Therefore, the ERM hypothesis of **w** on a dataset of *m* samples has weights\n",
    "$$\n",
    "{\\bf w} = \\underset{\\bf w}{argmin} (-\\frac{1}{m}\\sum_{i = 1}^m \\sum_{j = 1}^{k}\n",
    "\\left\\{\\begin{array}{lr}\n",
    "    \\log( h_{\\bf w}({\\bf x}_i)_j), & y_{i} = j\\\\\n",
    "    0, & \\text{otherwise} \\\\\n",
    "\\end{array}\\right\\} )\n",
    "$$\n",
    "To learn the ERM hypothesis, we need to perform gradient descent. The\n",
    "partial derivative of the loss function on a single data point\n",
    "$$\n",
    "\\frac{\\partial l_S(h_{\\bf w})}{\\partial {\\bf w}_{st}} =\n",
    "\\left\\{\\begin{array}{lr}\n",
    "    h_{\\bf w}({\\bf x})_s - 1, & y = s\\\\\n",
    "    h_{\\bf w}({\\bf x})_s, & \\text{otherwise} \\\\\n",
    "    \\end{array}\\right\\}\n",
    "    {\\bf x}_t\n",
    "$$\n",
    "With respect to a single row in the weights matrix, ${\\bf w}_s$,\n",
    "the partial derivative of the loss is\n",
    "$$\n",
    "\\frac{\\partial l_S(h_{\\bf w})}{\\partial {\\bf w}_{s}} =\n",
    "\\left\\{\\begin{array}{lr}\n",
    "    h_{\\bf w}({\\bf x})_s - 1, & y = s\\\\\n",
    "    h_{\\bf w}({\\bf x})_s, & \\text{otherwise} \\\\\n",
    "    \\end{array}\\right\\}\n",
    "    {\\bf x}\n",
    "$$\n",
    "You will need to descend this gradient to update the weights of your\n",
    "Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stochastic Gradient Descent**\n",
    "\n",
    "You will be using Stochastic Gradient Descent (SGD) to train your\n",
    "`LogisticRegression` model. Below, we have provided pseudocode for SGD\n",
    "on a sample *S*:\n",
    "\n",
    "$\\text{initialize parameters } {\\bf w}\\text{, learning rate } \\alpha \\text{, and batch size b}$  <br />\n",
    "$\\quad \\text{converge = False}$ <br />\n",
    "$\\quad \\text{while not converge:}$ <br />\n",
    "$\\quad \\quad\t\\text{epoch + 1}$ <br />\n",
    "$\\quad \\quad\t\\text{shuffle training examples}$ <br />\n",
    "$\\quad \\quad\t\\text{calculate last epoch loss}$ <br />\n",
    "$\\quad \\quad\t\\text{for } i = 0,1,...,\\lceil{n_{examples}/b}\\rceil-1: \\text{-- iterate over batches:}$ <br />\n",
    "$\\quad \\quad \\quad X_{batch} = X[i \\cdot b: (i+1) \\cdot b] \\text{ -- select the X in the current batch}$ <br />\n",
    "$\\quad \\quad \\quad {\\bf y}_{batch} = {\\bf y}[i \\cdot b: (i+1) \\cdot b] \\text{ -- select the labels in the current batch}$ <br />\n",
    "$\\quad \\quad \\quad \\text{initialize } \\nabla L_{{\\bf w}} \\text{ to be a matrix of zeros}$ <br />\n",
    "$\\quad \\quad \\quad \\text{for each pair of training data point } ({\\bf x},y)\\in (X_{batch}, {\\bf y}_{batch}):$ <br />\n",
    "$\\quad \\quad \\quad \\quad \\text{for }j = 0,1,..., n_{classes} - 1:$ <br />\n",
    "$\\quad \\quad \\quad \\quad \\quad \\text{-- calculate the partial derivative of the loss with respect to}$ <br />\n",
    "$\\quad \\quad \\quad \\quad \\quad \\text{-- a single row in the weights matrix}$ <br />\n",
    "$\\quad \\quad \\quad \\quad \\quad \\text{if }y = j: \\nabla L_{{\\bf w}_j} \\text{ += } \n",
    "(softmax(\\langle {\\bf w}_j, {\\bf x} \\rangle) - 1) \\cdot {\\bf x} $ <br />\n",
    "$\\quad \\quad \\quad \\quad \\quad \\text{else: }\\nabla L_{{\\bf w}_j} \\text{ += } (softmax(\\langle {\\bf w}_j, {\\bf x} \\rangle) ) \\cdot {\\bf x}$ <br />\n",
    "$\\quad \\quad \\quad {\\bf w} = {\\bf w} - \\frac{\\alpha \\nabla L_{\\bf w}}{len(X_{batch})} \\text{ -- update the weights}$ <br />\n",
    "$\\quad \\quad \\text{calculate this epoch loss}$ <br />\n",
    "$\\quad \\quad \\text{if |Loss}(X,{\\bf y})_{this-epoch}-Loss(X,{\\bf y})_{last-epoch}| <  \\text{CONV-THRESHOLD: }$ <br />\n",
    "$\\quad \\quad \\quad \\text{converge = True -- break the loop if loss converged}$\n",
    "\n",
    "\n",
    " **Hints**: Consistent with the notation in the lecture, ${\\bf w}$ are\n",
    "initialized as a *k* x *d* matrix, where *k* is the number of classes\n",
    "and *d* is the number of features (with the bias term). With *n* as the\n",
    "number of examples, *X* is a *n* x *d* matrix, and ${\\bf y}$ is a vector\n",
    "of length *n*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tuning Parameters**\n",
    "\n",
    "Convergence is achieved when the change in loss between iterations is\n",
    "some small value. Usually, this value will be very close to but not\n",
    "equal to zero, so it is up to you to tune this threshold value to best\n",
    "optimize your model's performance. Typically, this number will be some\n",
    "magnitude of 10<sup>-x</sup>, where you experiment with *x*. Note that\n",
    "when calculating the loss for checking convergence, you should be\n",
    "calculating the loss for the entire dataset, not for a single batch\n",
    "(i.e., at the end of every epoch).  \n",
    "  \n",
    "You will also be tuning batch size (and one of the report questions\n",
    "addresses the impact of batch size on model performance). In order to\n",
    "reach the accuracy threshold, you will need to tune both parameters. *$\\alpha$*\n",
    "would typically be tuned during the training process, but we are fixing\n",
    "*$\\alpha$* = 0.03 for this assignment. **Please do not change *$\\alpha$* in your\n",
    "code**.  \n",
    "  \n",
    "You can tune the batch size and convergence threshold in `Main`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extra: Numpy Shortcuts**\n",
    "\n",
    "While optional, there are many numpy shortcuts and functions that can make your code cleaner. We encourage you to look up numpy documentation and learn new functions.\n",
    "\n",
    "Some useful shortcuts:\n",
    "* `A @ B` is a shortcut for `np.matmul(A, B)`\n",
    "* `X.T` is a shortcut for `np.transpose(X)`\n",
    "* `X.shape` is a shortcut for `np.shape(X)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    '''\n",
    "    Apply softmax to an array\n",
    "    @params:\n",
    "        x: the original array\n",
    "    @return:\n",
    "        an array with softmax applied elementwise.\n",
    "    '''\n",
    "    e = np.exp(x - np.max(x))\n",
    "    return (e + 1e-6) / (np.sum(e) + 1e-6)\n",
    "\n",
    "class LogisticRegression:\n",
    "    '''\n",
    "    Multiclass Logistic Regression that learns weights using \n",
    "    stochastic gradient descent.\n",
    "    '''\n",
    "    def __init__(self, n_features, n_classes, batch_size, conv_threshold):\n",
    "        '''\n",
    "        Initializes a LogisticRegression classifer.\n",
    "        @attrs:\n",
    "            n_features: the number of features in the classification problem\n",
    "            n_classes: the number of classes in the classification problem\n",
    "            weights: The weights of the Logistic Regression model\n",
    "            alpha: The learning rate used in stochastic gradient descent\n",
    "        '''\n",
    "        self.n_classes = n_classes\n",
    "        self.n_features = n_features\n",
    "        self.weights = np.zeros((n_classes, n_features + 1)) # An extra row added for the bias\n",
    "        self.alpha = 0.03  # DO NOT TUNE THIS PARAMETER\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_threshold = conv_threshold\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        Trains the model using stochastic gradient descent\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            num_epochs: integer representing the number of epochs taken to reach convergence\n",
    "        '''\n",
    "        # [TODO]\n",
    "        num_samples = X.shape[0] # get the number of samples\n",
    "        num_epochs = 0 # let the number of epochs be 0 initially\n",
    "        prev_loss = self.loss(X, Y)  # calculate the initial loss\n",
    "        np.random.seed(42) # set seed for reproducibility\n",
    "        while True:\n",
    "            num_epochs += 1 # number of epochs increases 1 for each loop\n",
    "            indices = np.arange(num_samples) # get all the indices of samples\n",
    "            np.random.shuffle(indices) # shuffle the indices\n",
    "            # shuffle the samples according to the shuffled indices\n",
    "            X = X[indices]\n",
    "            Y = Y[indices]\n",
    "\n",
    "            # run over each batch\n",
    "            for i in range(0, num_samples, self.batch_size):\n",
    "                # get the current batch\n",
    "                X_batch = X[i:i+self.batch_size]\n",
    "                Y_batch = Y[i:i+self.batch_size]\n",
    "                # intial the gradient to be zero matrix\n",
    "                grad = np.zeros_like(self.weights)\n",
    "\n",
    "                # run over each sample in the batch\n",
    "                for x, y in zip(X_batch, Y_batch): \n",
    "                    # calculate ⟨w,x⟩\n",
    "                    scores = np.dot(self.weights, x)\n",
    "                    # calculate the softmax probabilities\n",
    "                    probs = softmax(scores)\n",
    "\n",
    "                    # run over each class to calculate the gradient\n",
    "                    for j in range(self.n_classes):\n",
    "                        if j == y:\n",
    "                            grad[j] += (probs[j] - 1) * x\n",
    "                        else:\n",
    "                            grad[j] += probs[j] * x\n",
    "\n",
    "                # calculate the average gradient for the batch\n",
    "                grad /= len(X_batch)\n",
    "                # update weights\n",
    "                self.weights -= self.alpha * grad\n",
    "\n",
    "            # calculate the current loss after each epoch\n",
    "            curr_loss = self.loss(X, Y)\n",
    "            # check for convergence\n",
    "            if abs(prev_loss - curr_loss) < self.conv_threshold: # if covergence, then break the loop\n",
    "                break\n",
    "            # if not convergence, then set the previous loss to current loss for next epoch\n",
    "            prev_loss = curr_loss\n",
    "\n",
    "        return num_epochs\n",
    "\n",
    "    \n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the total log loss on some dataset (X, Y), divided by the number of examples.\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            A float number which is the average loss of the model on the dataset\n",
    "        '''\n",
    "        # [TODO]\n",
    "        total_loss = 0 # set the total loss to be 0 initially\n",
    "        n = X.shape[0] # number of samples\n",
    "\n",
    "        # run over each sample to calculate the loss\n",
    "        for i in range(n):\n",
    "            x = X[i] # x is the i-th sample\n",
    "            y = Y[i] # y is the label of the i-th sample\n",
    "            # calculate ⟨w,x⟩\n",
    "            scores = np.dot(self.weights, x)\n",
    "            # calculate the softmax probabilities for each class\n",
    "            probs = softmax(scores)\n",
    "            # accumulate the loss\n",
    "            total_loss += -np.log(probs[y] + 1e-6)\n",
    "        # return the average loss\n",
    "        return total_loss / n\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Compute predictions based on the learned weigths and examples X\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each row in X containing the predicted class.\n",
    "        '''\n",
    "        # [TODO]\n",
    "        predictions = [] # set a list to store the predictions\n",
    "        # run over each sample to make predictions\n",
    "        for x in X:\n",
    "            # calculate ⟨w,x⟩\n",
    "            scores = np.dot(self.weights, x)\n",
    "            # calculate the softmax probabilities for each class\n",
    "            probs = softmax(scores)\n",
    "            # get the class with the highest probability\n",
    "            pred_label = np.argmax(probs)\n",
    "            # append the predicted label to the predictions list\n",
    "            predictions.append(pred_label)\n",
    "        # return the predictions as a numpy array\n",
    "        return np.array(predictions)\n",
    "\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        '''\n",
    "        Outputs the accuracy of the trained model on a given testing dataset X and labels Y.\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            a float number indicating accuracy (between 0 and 1)\n",
    "        '''\n",
    "        # [TODO]\n",
    "        preds = self.predict(X)\n",
    "        # calculate the number of correct predictions\n",
    "        correct = np.sum(preds == Y)\n",
    "        # return the accuracy\n",
    "        return correct / len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "# Sets random seed for testing purposes\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Creates Test Model with 2 predictors, 2 classes, a Batch Size of 5 and a Threshold of 1e-2\n",
    "test_model1 = LogisticRegression(2, 2, 5, 1e-2)\n",
    "\n",
    "# Creates Test Data\n",
    "x_bias = np.array([[0,4,1], [0,3,1], [5,0,1], [4,1,1], [0,5,1]])\n",
    "y = np.array([0,0,1,1,0])\n",
    "x_bias_test = np.array([[0,0,1], [-5,3,1], [9,0,1], [1,0,1], [6,-7,1]])\n",
    "y_test = np.array([0,0,1,0,1])\n",
    "\n",
    "# Creates Test Model with 2 predictors, 1 classes, a Batch Size of 1 and a Threshold of 1e-2\n",
    "test_model2 = LogisticRegression(2, 3, 1, 1e-2)\n",
    "\n",
    "# Creates Test Data\n",
    "x_bias2 = np.array([[0,0,1], [0,3,1], [4,0,1], [6,1,1], [0,1,1], [0,4,1]])\n",
    "y2 = np.array([0,1,2,2,0,1])\n",
    "x_bias_test2 = np.array([[0,0,1], [-5,3,1], [9,0,1], [1,0,1]])\n",
    "y_test2 = np.array([0,1,2,0])\n",
    "\n",
    "\n",
    "# Test Model Loss\n",
    "assert test_model1.loss(x_bias, y) == pytest.approx(0.693, .001) # Checks if answer is within .001\n",
    "assert test_model2.loss(x_bias2, y2) == pytest.approx(1.099, .001) # Checks if answer is within .001\n",
    "\n",
    "# Test Train Model and Checks Model Weights\n",
    "assert test_model1.train(x_bias, y) == 14\n",
    "assert test_model1.weights == pytest.approx(\n",
    "    np.array([[-0.218, 0.231, 0.0174], [ 0.218, -0.231, -0.0174]]), 0.01) # Answer within .01\n",
    "\n",
    "assert test_model2.train(x_bias, y) == 9\n",
    "assert test_model2.weights == pytest.approx(\n",
    "    np.array([[-0.300,  0.560,  0.093], [ 0.523, -0.257,  0.032], [-0.226, -0.304, -0.123]]), .05) \n",
    "\n",
    "# Test Model Predict\n",
    "assert (test_model1.predict(x_bias_test) == np.array([0., 0., 1., 1., 1.])).all()\n",
    "assert (test_model2.predict(x_bias_test2) == np.array([0, 0, 1, 1])).all()\n",
    "\n",
    "# Test Model Accuracy\n",
    "assert test_model1.accuracy(x_bias_test, y_test) == .8\n",
    "assert test_model2.accuracy(x_bias_test2, y_test2) == .25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 94.8%\n",
      "Number of Epochs: 230\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_FILE_NAME = 'normalized_data.csv'\n",
    "# Question 2\n",
    "# DATA_FILE_NAME = 'unnormalized_data.csv'\n",
    "# Question 3\n",
    "# DATA_FILE_NAME = 'normalized_data_nosens.csv'\n",
    "\n",
    "CENSUS_FILE_PATH = DATA_FILE_NAME\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 10  # [TODO]: tune this parameter\n",
    "CONV_THRESHOLD = 0.00001 # [TODO]: tune this parameter\n",
    "\n",
    "def import_census(file_path):\n",
    "    '''\n",
    "        Helper function to import the census dataset\n",
    "        @param:\n",
    "            train_path: path to census train data + labels\n",
    "            test_path: path to census test data + labels\n",
    "        @return:\n",
    "            X_train: training data inputs\n",
    "            Y_train: training data labels\n",
    "            X_test: testing data inputs\n",
    "            Y_test: testing data labels\n",
    "    '''\n",
    "    data = np.genfromtxt(file_path, delimiter=',', skip_header=False)\n",
    "    X = data[:, :-1]\n",
    "    Y = data[:, -1].astype(int)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def test_logreg():\n",
    "    X_train, Y_train, X_test, Y_test = import_census(CENSUS_FILE_PATH)\n",
    "    num_features = X_train.shape[1]\n",
    "\n",
    "    # Add a bias\n",
    "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "    ### Logistic Regression ###\n",
    "    model = LogisticRegression(num_features, NUM_CLASSES, BATCH_SIZE, CONV_THRESHOLD)\n",
    "    num_epochs = model.train(X_train_b, Y_train)\n",
    "    acc = model.accuracy(X_test_b, Y_test) * 100\n",
    "    print(\"Test Accuracy: {:.1f}%\".format(acc))\n",
    "    print(\"Number of Epochs: \" + str(num_epochs))\n",
    "\n",
    "# Set random seeds. DO NOT CHANGE THIS IN YOUR FINAL SUBMISSION.\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "test_logreg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model (Cont'd)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy: 76.2%\n",
      "Average Number of Epochs: 2.0\n"
     ]
    }
   ],
   "source": [
    "### test your model on the census dataset\n",
    "X_train, Y_train, X_test, Y_test = import_census(CENSUS_FILE_PATH)\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# Add a bias\n",
    "X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "# Logistic Regression, average accross 10 random states\n",
    "random.seed(0)\n",
    "num_states = 10\n",
    "num_epochs, test_accuracies = [], []\n",
    "\n",
    "for _ in range(num_states):\n",
    "    random_state = random.randint(1, 1000)\n",
    "    random.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    model = LogisticRegression(num_features, n_classes=3, batch_size=1, conv_threshold=0.1)\n",
    "    num_epochs.append(model.train(X_train_b, Y_train))\n",
    "    test_accuracies.append(model.accuracy(X_test_b, Y_test) * 100)\n",
    "\n",
    "avg_test_accuracy = sum(test_accuracies) / num_states\n",
    "avg_num_epochs = sum(num_epochs) / num_states\n",
    "print(\"Average Test Accuracy: {:.1f}%\".format(avg_test_accuracy))\n",
    "print(\"Average Number of Epochs: \" + str(avg_num_epochs))\n",
    "\n",
    "assert 1.5 < avg_num_epochs < 2.5\n",
    "assert 75 < avg_test_accuracy < 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Report Questions (15 points)**\n",
    "\n",
    "### **Question 1**\n",
    "\n",
    "Make sure that you have implemented a variable batch size using the\n",
    "constructor given for `LogisticRegression`. Try different batch\n",
    "sizes ([1, 8, 64, 512, 4096] - there are ~5700 points in the dataset), and try different convergence thresholds ([1e-1, 1e-2, 1e-3]) in the cell below. Visualize the accuracy and number of epochs taken to converge.\n",
    "\n",
    "Answer the following questions:\n",
    "-   What tradeoffs exist between good accuracy and quick\n",
    "    convergence?\n",
    "-    Why do you think the batch size led to the results you received?\n",
    "\n",
    "Fill in the `generate_array()` and `generate_heatmap()` functions so you can visualize how accuracy and number of epochs taken changes as we change batch size and convergence threshold. Fill out BATCH_SIZE_ARR and CONV_THRESHOLD_ARR with the values described above.\n",
    "\n",
    "-   **generate_array()** should loop through both BATCH_SIZE_ARR and CONV_THRESHOLD_ARR to populate `epoch_arr` and `acc_arr`. Make sure to round `acc_arr` to 2 decimal places before returning (Hint: `np.round`).\n",
    "        \n",
    "-   **generate_heatmap()** should create a matplotlib heatmap of the arrays. You should label the axis and title of each plot using BATCH_SIZE_ARR and CONV_THRESHOLD_ARR. It might be helpful to look at Matplotlib's guide for heatmaps: https://matplotlib.org/stable/gallery/images_contours_and_fields/image_annotated_heatmap.html\n",
    "\n",
    "**Hint:** Runs with large batch sizes and low convergence thresholds might take several minutes to half an hour to complete. We recommend that you develop the code below with a small subset of the parameters (e.g., batch size of [1,2,4] and conv_threshold of [1e-1, 1e-2]). Once your code works and your figures look good, rerun everything with the batch size and conv_threshold values described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "BATCH_SIZE_ARR = [1, 8, 64, 512, 4096] # [TODO]: try different values\n",
    "CONV_THRESHOLD_ARR = [1e-1, 1e-2, 1e-3]  # [TODO]: try different values\n",
    "\n",
    "def generate_array():\n",
    "    '''\n",
    "        Runs the logistic regression model on different batch sizes and\n",
    "        convergence thresholds to populate arrays for accuracy and number of epochs taken.\n",
    "        @return:\n",
    "            epoch_arr: 2D array of epochs taken, for each batch size and conv threshold\n",
    "            acc_arr: 2D array of accuracies, for each batch size and conv threshold\n",
    "    '''\n",
    "    X_train, Y_train, X_test, Y_test = import_census(CENSUS_FILE_PATH)\n",
    "    num_features = X_train.shape[1]\n",
    "\n",
    "    # Add a bias\n",
    "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "    # Initializes the accuracy and epoch arrays\n",
    "    acc_arr = np.zeros((len(BATCH_SIZE_ARR), len(CONV_THRESHOLD_ARR)))\n",
    "    epoch_arr = np.zeros((len(BATCH_SIZE_ARR), len(CONV_THRESHOLD_ARR)))\n",
    "\n",
    "    ### Populate arrays ###\n",
    "    # [TODO]\n",
    "        # run over each batch size\n",
    "    for i, batch_size in enumerate(BATCH_SIZE_ARR):\n",
    "        # run over each convergence threshold\n",
    "        for j, conv_threshold in enumerate(CONV_THRESHOLD_ARR):\n",
    "            # create a logistic regression model for each combination of batch size and conv threshold\n",
    "            # we defined NUM_CLASSES = 3 previously\n",
    "            model = LogisticRegression(num_features, NUM_CLASSES, batch_size, conv_threshold)\n",
    "            # get the number of epoch\n",
    "            num_epochs = model.train(X_train_b, Y_train)\n",
    "            # calcuate the accuracy\n",
    "            acc = model.accuracy(X_test_b, Y_test) * 100\n",
    "            # save the number of epochs and accuracy to the corresponding arrays\n",
    "            epoch_arr[i, j] = num_epochs\n",
    "            acc_arr[i, j] = round(acc, 2)  # round to 2 decimal places\n",
    "    \n",
    "    return epoch_arr, acc_arr\n",
    "\n",
    "\n",
    "def generate_heatmap(arr, name):\n",
    "    '''\n",
    "        Generates a matplotlib heatmap for an array\n",
    "        convergence thresholds to populate arrays for accuracy and number of epochs taken.\n",
    "        @param:\n",
    "            arr: 2D array to generate heatmap of\n",
    "            name: title of the plot (Hint: use plt.title)\n",
    "        @return:\n",
    "            None\n",
    "    '''\n",
    "    # [TODO]\n",
    "    plt.figure()\n",
    "    plt.imshow(arr, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar()  # display the color bar on the right\n",
    "    # set x ticks and labels\n",
    "    plt.xticks(ticks=range(len(CONV_THRESHOLD_ARR)), labels=CONV_THRESHOLD_ARR)\n",
    "    # set y ticks and labels\n",
    "    plt.yticks(ticks=range(len(BATCH_SIZE_ARR)), labels=BATCH_SIZE_ARR)\n",
    "    # set x and y labels\n",
    "    plt.xlabel('Convergence Threshold')\n",
    "    plt.ylabel('Batch Size')\n",
    "    # set title\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "epoch_arr, acc_arr = generate_array()\n",
    "generate_heatmap(epoch_arr, \"Epochs\")\n",
    "generate_heatmap(acc_arr, \"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "\n",
    "-   What tradeoffs exist between good accuracy and quick convergence?\n",
    "\n",
    "Small batch size and small convergence threshold lead to good accuracy; large batch size and large convergence threshold lead to bad accuracy. However, small convergence threshold lead to large epochs (slow convergence) and lead to good accuracy; large convergence threshold lead to small epochs (quick convergence) and lead to bad accuracy. Then we need to balance batch size and convergence threshold in order to balance good accuracy and quick convergence.\n",
    "\n",
    "-    Why do you think the batch size led to the results you received?\n",
    "\n",
    "Small batch size leads to good accuracy because each update uses only a few samples, which introduces some noise in the gradient. This “shaking” helps the model escape bad solutions and find better ones, improving test accuracy. But large batch size leads to bad accuracy because each update uses many samples, making the gradient very stable. This can make the model get stuck in suboptimal solutions and overfit the training data, lowering test accuracy.\n",
    "\n",
    "Small batch sizes mean that only a small number of examples are used to update parameters at a time, which can lead to gradient fluctuations and require more rounds of training to converge. Consequently, the number of epochs may be higher. Large batch sizes, which use a large number of examples at a time, stabilize gradients and sometimes allow for rapid convergence. However, they can also lead to suboptimal solutions due to a lack of exploration, resulting in erratic epoch times. Gradient fluctuations and the convergence threshold jointly influence the number of training rounds, so the relationship between batch size and epochs is sometimes directly proportional and sometimes inversely proportional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "Try to run the model with `unnormalized_data.csv` instead of\n",
    "`normalized_data.csv`. Report your findings when running the model\n",
    "on the unnormalized data. In a few short sentences, explain what\n",
    "normalizing the data does and why it affected your model's\n",
    "performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**  \n",
    "I set BATCH_SIZE = 10 and CONV_THRESHOLD = 0.00001.\n",
    "\n",
    "When I run the normalized_data.csv, Test Accuracy: 94.8%, Number of Epochs: 230, Average Test Accuracy: 76.2% and Average Number of Epochs: 2.0\n",
    "\n",
    "However when the dataset is changed to normalized_data.csv, Test Accuracy: 34.1%, Number of Epochs: 11, Average Test Accuracy: 33.6% and Average Number of Epochs: 9.0 which raise AssertError (because assert 1.5 < avg_num_epochs < 2.5; assert 75 < avg_test_accuracy < 80)\n",
    "\n",
    "Thus, data normalization scales all features to a similar range, making gradient descent more stable. Without normalization, the scales of features vary widely, and the gradients are dominated by large features, resulting in slow model convergence and low accuracy. While normalization improves training efficiency and predictive performance, models often perform poorly on unnormalized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 3**\n",
    "Try the model with `normalized_data_nosens.csv`; in this data file,\n",
    "we have removed the `race` and `sex` attributes. Report your\n",
    "findings on the accuracy of your model on this dataset (averaging\n",
    "over many random seeds here may be useful). Can we make any\n",
    "conclusion based on these accuracy results about whether there is a\n",
    "correlation between sex/race and education level? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** \n",
    "\n",
    "I set BATCH_SIZE = 10 and CONV_THRESHOLD = 0.00001.\n",
    "\n",
    "When I run the normalized_data.csv, Test Accuracy: 94.8%, Number of Epochs: 230, Average Test Accuracy: 76.2% and Average Number of Epochs: 2.0\n",
    "\n",
    "However,When I run the normalized_data_nosens.csv, Test Accuracy: 94.0%, Number of Epochs: 239, Average Test Accuracy: 76.9% and Average Number of Epochs: 2.0.\n",
    "\n",
    "After removing race and sex, the model's average test accuracy barely decreased (from 76.2% to 76.9%), and the number of training epochs remained largely unchanged. This suggests that race and sex contribute very little to predicting education level, and the model achieves nearly the same accuracy without these sensitive features. Therefore, model accuracy alone cannot directly prove that race or sex are uncorrelated with education level, but they are not the primary determinants of model predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
